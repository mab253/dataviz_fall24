{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## SALIENCY MAPS ON PRE-TRAINED MODELS\n",
        "\n",
        "# ðŸ“š bring in my libraries!\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# our old friend ...\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check that tensorflow installed correctly\n",
        "print('tensorflow {}'.format(tf.__version__))\n",
        "\n"
      ],
      "metadata": {
        "id": "wyf5Nm-JAYW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16 is a pre-trained model, available via tensorflow\n",
        "# VGG16 is a neural network architecture particularly good @ image classification\n",
        "# keras = the API that talks to tensorflow\n",
        "\n",
        "model = keras.applications.VGG16(weights='imagenet')"
      ],
      "metadata": {
        "id": "q17DaF_FAqc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# gives me the summary of the model we've downloaded (VGG16)\n",
        "# check it out: there are 16 \"layers\" to the model\n",
        "# each layer in a neural network represents a series of operations\n",
        "# performed on the input data as it propagates through the network\n",
        "# on its way to producing the final output"
      ],
      "metadata": {
        "id": "5TaadP51Auhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in an image!\n",
        "# the dimensions here (224x224 pixels) need to match what the pre-trained model expects\n",
        "\n",
        "_img = keras.preprocessing.image.load_img('/content/cat_sq.jpg',target_size=(224,224))\n",
        "plt.imshow(_img)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eGYK4ClxAxG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess image to get it into the right format for the model\n",
        "\n",
        "img = keras.preprocessing.image.img_to_array(_img)\n",
        "# this line converts to image to a NumPy array of pixels\n",
        "# x, y, and RGB value - for every pixel (224 x 224 x 3)\n",
        "\n",
        "img = img.reshape((1, *img.shape))\n",
        "# more preprocessing, adding a dimension (\"batch\") that the model expects\n",
        "# a 4D array: (1 x 224 x 224 x 3)\n",
        "\n",
        "\n",
        "y_pred = model.predict(img)\n",
        "print(y_pred)\n",
        "# an output of predictions from the image classification model\n",
        "# each # is a \"score\" of how much the image is likely to be a certain class\n",
        "# https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "# 1000 possible classes"
      ],
      "metadata": {
        "id": "eAO6m5H4A7WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this loops through the top 5 highest predicions in y_pred\n",
        "# and links them to the associated categories (decode_predictions function is built-in)\n",
        "# this prints out what the model \"thinks\" the image is\n",
        "decoded_predictions = decode_predictions(y_pred, top=5)[0]\n",
        "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
        "    print(f\"{i + 1}: {label} ({score:.2f})\")\n"
      ],
      "metadata": {
        "id": "Ut8v4ng5DOw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BIG PICTURE: we are trying to get a measure of which PARTS of the image\n",
        "# are most important, predictive features for the model\n",
        "\n",
        "# gradient here is a measure of CHANGE: how much will the output change if you adjust the input?\n",
        "# remember that gradient is looking to minimize the LOSS function\n",
        "\n",
        "# convert input image to tf.Variable, a multi-dimensional array for the model\n",
        "images = tf.Variable(img, dtype=float)\n",
        "\n",
        "# gradientTape() keeps track of all these gradients\n",
        "# we're looking for the most likely class again (just in a different format)\n",
        "with tf.GradientTape() as tape:\n",
        "    pred = model(images, training=False)\n",
        "    class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]\n",
        "    loss = pred[0][class_idxs_sorted[0]]\n",
        "\n",
        "grads = tape.gradient(loss, images)\n"
      ],
      "metadata": {
        "id": "rU3q8F5tBRuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get absolute value\n",
        "dgrad_abs = tf.math.abs(grads)"
      ],
      "metadata": {
        "id": "_r1_aFSbBUbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for the maximum in the list of absolute gradient values\n",
        "# highest gradient = most sensitive to change\n",
        "dgrad_max_ = np.max(dgrad_abs, axis=3)[0]"
      ],
      "metadata": {
        "id": "0zTxcDD0BWUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize to range between 0 and 1\n",
        "arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
        "grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n"
      ],
      "metadata": {
        "id": "0uUBtZEjBYiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW! we plot\n",
        "\n",
        "# hello again, matplotlib ...\n",
        "fig, axes = plt.subplots(1,2,figsize=(14,5))\n",
        "\n",
        "# the first subplot = the input image\n",
        "axes[0].imshow(_img)\n",
        "\n",
        "# the second subplot is the SALIENCY MAP:\n",
        "# MOST IMPORTANT pixels highlighted for the models prediction!\n",
        "i = axes[1].imshow(grad_eval,cmap=\"viridis\",alpha=0.8)\n",
        "fig.colorbar(i)\n"
      ],
      "metadata": {
        "id": "sEdgRvc2Ba-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wJAiOgqiuELE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also show ACTIVATION MAP:\n",
        "# what the process looks like for every \"layer\" in the model\n",
        "\n",
        "# make a list of all the model's layers\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# loop through each layer and visualize the activation map\n",
        "for layer_name in layer_names:\n",
        "    # create a model that outputs the activation map for the current layer\n",
        "    activation_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "\n",
        "    # get the activation map for the input image\n",
        "    activations = activation_model.predict(img)\n",
        "    print(activations.shape)\n",
        "\n",
        "    if len(activations.shape) == 4:\n",
        "      # display the activation map if layer is visual\n",
        "      plt.figure()\n",
        "      plt.imshow(activations[0, :, :, 0], cmap='viridis')\n",
        "      plt.title(f'Activation Map for Layer: {layer_name}')\n",
        "      plt.colorbar()\n",
        "\n",
        "plt.show()\n",
        "# show 'em all!\n"
      ],
      "metadata": {
        "id": "Am1kAW-7FL06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“¸ Citations:\n",
        "\n",
        "- https://usmanr149.github.io/urmlblog/cnn/2020/05/01/Salincy-Maps.html - (much of the code/idea for this demo comes from this post!)\n",
        "- https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a (image classification categories)\n",
        "- https://towardsdatascience.com/saliency-map-using-pytorch-68270fe45e80"
      ],
      "metadata": {
        "id": "z5k8AlBWGfxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## BELOW: code for just 1 layer's activation map\n",
        "\n",
        "# Define the layer for which you want to visualize the activation map\n",
        "layer_name = 'block3_conv1'  # Change this to the desired layer\n",
        "\n",
        "# Create a model that outputs the activation map for the specified layer\n",
        "activation_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "\n",
        "# Get the activation map for the input image\n",
        "activations = activation_model.predict(img)\n",
        "\n",
        "# Visualize the activation map\n",
        "plt.imshow(activations[0, :, :, 0], cmap='viridis')\n",
        "plt.title(f'Activation Map for Layer: {layer_name}')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_4Meu-mxE0FT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}